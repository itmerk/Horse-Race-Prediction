{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary Module\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score, classification_report\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_predict\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, cross_val_predict\n",
    "from scipy.stats import uniform\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.read_csv(r\"D:\\Data Science\\Projects\\My Projects\\Project 12\\Horse Race Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rid</th>\n",
       "      <th>horseName</th>\n",
       "      <th>age</th>\n",
       "      <th>saddle</th>\n",
       "      <th>decimalPrice</th>\n",
       "      <th>isFav</th>\n",
       "      <th>trainerName</th>\n",
       "      <th>jockeyName</th>\n",
       "      <th>position</th>\n",
       "      <th>positionL</th>\n",
       "      <th>...</th>\n",
       "      <th>metric</th>\n",
       "      <th>countryCode</th>\n",
       "      <th>ncond</th>\n",
       "      <th>class</th>\n",
       "      <th>distanceFeet</th>\n",
       "      <th>hours</th>\n",
       "      <th>minutes</th>\n",
       "      <th>raceYear</th>\n",
       "      <th>raceMonth</th>\n",
       "      <th>raceDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>222908.0</td>\n",
       "      <td>3357.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>5838.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3720.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.56</td>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>235573.0</td>\n",
       "      <td>107872.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3061.0</td>\n",
       "      <td>543.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>4725.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>235573.0</td>\n",
       "      <td>890.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4054.0</td>\n",
       "      <td>5895.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4725.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80678.0</td>\n",
       "      <td>126232.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1172.0</td>\n",
       "      <td>4387.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3519.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>310795.0</td>\n",
       "      <td>122905.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4104.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>4827.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.84</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        rid  horseName   age  saddle  decimalPrice  isFav  trainerName  \\\n",
       "0  222908.0     3357.0  10.0    11.0      0.066667    0.0        261.0   \n",
       "1  235573.0   107872.0   9.0     2.0      0.090909    0.0       3061.0   \n",
       "2  235573.0      890.0   9.0     3.0      0.125000    0.0       4054.0   \n",
       "3   80678.0   126232.0   6.0     5.0      0.047619    0.0       1172.0   \n",
       "4  310795.0   122905.0   7.0     1.0      0.100000    0.0       4104.0   \n",
       "\n",
       "   jockeyName  position  positionL  ...  metric  countryCode  ncond  class  \\\n",
       "0      5838.0       5.0        8.0  ...  3720.5          0.0    9.0    4.0   \n",
       "1       543.0       3.0        2.5  ...  4725.5          0.0    9.0    3.0   \n",
       "2      5895.0       4.0        5.0  ...  4725.5          0.0    9.0    3.0   \n",
       "3      4387.0       3.0        6.0  ...  3519.5          0.0    9.0    4.0   \n",
       "4       319.0       3.0        0.5  ...  4827.0          0.0    9.0    4.0   \n",
       "\n",
       "   distanceFeet  hours  minutes  raceYear  raceMonth  raceDate  \n",
       "0          7.56    3.0     25.0       1.0        1.0       1.0  \n",
       "1         10.06    1.0     45.0       1.0        1.0       1.0  \n",
       "2         10.06    1.0     45.0       1.0        1.0       1.0  \n",
       "3          7.06    1.0     30.0       1.0        1.0       2.0  \n",
       "4          9.84    3.0     10.0       1.0        1.0       4.0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rid', 'horseName', 'age', 'saddle', 'decimalPrice', 'isFav',\n",
       "       'trainerName', 'jockeyName', 'position', 'positionL', 'dist',\n",
       "       'weightSt', 'weightLb', 'RPR', 'TR', 'OR', 'father', 'mother',\n",
       "       'gfather', 'runners', 'margin', 'weight', 'res_win', 'res_place',\n",
       "       'course', 'title', 'rclass', 'ages', 'condition', 'winningTime',\n",
       "       'prize', 'metric', 'countryCode', 'ncond', 'class', 'distanceFeet',\n",
       "       'hours', 'minutes', 'raceYear', 'raceMonth', 'raceDate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data['res_place'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25453"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_data[merged_data['res_place'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173604"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_data[merged_data['res_place'] == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159245, 40) (39812, 40) (159245,) (39812,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores:\n",
      "Accuracy: 0.8722848441081352\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      1.00      0.93    138907\n",
      "         1.0       0.00      0.00      0.00     20338\n",
      "\n",
      "    accuracy                           0.87    159245\n",
      "   macro avg       0.44      0.50      0.47    159245\n",
      "weighted avg       0.76      0.87      0.81    159245\n",
      "\n",
      "\n",
      "Testing Scores:\n",
      "Accuracy: 0.8715211494021903\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      1.00      0.93     34697\n",
      "         1.0       0.00      0.00      0.00      5115\n",
      "\n",
      "    accuracy                           0.87     39812\n",
      "   macro avg       0.44      0.50      0.47     39812\n",
      "weighted avg       0.76      0.87      0.81     39812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)\n",
    "\n",
    "# Train the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training and testing sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print training scores\n",
    "print(\"Training Scores:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_train, y_train_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_train, y_train_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_train, y_train_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_train, y_train_pred)}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "# Calculate and print testing scores\n",
    "print(\"\\nTesting Scores:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_test_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_test_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_test_pred)}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00    138907\n",
      "         1.0       1.00      1.00      1.00     20338\n",
      "\n",
      "    accuracy                           1.00    159245\n",
      "   macro avg       1.00      1.00      1.00    159245\n",
      "weighted avg       1.00      1.00      1.00    159245\n",
      "\n",
      "\n",
      "Testing Scores:\n",
      "Accuracy: 0.9996232291771325\n",
      "Precision: 0.9998039984319874\n",
      "Recall: 0.9972629521016618\n",
      "F1 Score: 0.9985318586669276\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     34697\n",
      "         1.0       1.00      1.00      1.00      5115\n",
      "\n",
      "    accuracy                           1.00     39812\n",
      "   macro avg       1.00      1.00      1.00     39812\n",
      "weighted avg       1.00      1.00      1.00     39812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "# Predict on the training and testing sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print training scores\n",
    "print(\"Training Scores:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_train, y_train_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_train, y_train_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_train, y_train_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_train, y_train_pred)}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "# Calculate and print testing scores\n",
    "print(\"\\nTesting Scores:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_test_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_test_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_test_pred)}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00    138907\n",
      "         1.0       1.00      1.00      1.00     20338\n",
      "\n",
      "    accuracy                           1.00    159245\n",
      "   macro avg       1.00      1.00      1.00    159245\n",
      "weighted avg       1.00      1.00      1.00    159245\n",
      "\n",
      "\n",
      "Testing Scores:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     34697\n",
      "         1.0       1.00      1.00      1.00      5115\n",
      "\n",
      "    accuracy                           1.00     39812\n",
      "   macro avg       1.00      1.00      1.00     39812\n",
      "weighted avg       1.00      1.00      1.00     39812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)\n",
    "\n",
    "# Initialize the GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "# Predict on the training and testing sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print training scores\n",
    "print(\"Training Scores:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_train, y_train_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_train, y_train_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_train, y_train_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_train, y_train_pred)}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "# Calculate and print testing scores\n",
    "print(\"\\nTesting Scores:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_test_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_test_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_test_pred)}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores:\n",
      "Accuracy: 0.8834814279883199\n",
      "Precision: 0.8834408602150537\n",
      "Recall: 0.10099321467204249\n",
      "F1 Score: 0.18126461633499535\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      1.00      0.94    138907\n",
      "         1.0       0.88      0.10      0.18     20338\n",
      "\n",
      "    accuracy                           0.88    159245\n",
      "   macro avg       0.88      0.55      0.56    159245\n",
      "weighted avg       0.88      0.88      0.84    159245\n",
      "\n",
      "\n",
      "Testing Scores:\n",
      "Accuracy: 0.882472621320205\n",
      "Precision: 0.8758620689655172\n",
      "Recall: 0.09931573802541545\n",
      "F1 Score: 0.1784021071115013\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      1.00      0.94     34697\n",
      "         1.0       0.88      0.10      0.18      5115\n",
      "\n",
      "    accuracy                           0.88     39812\n",
      "   macro avg       0.88      0.55      0.56     39812\n",
      "weighted avg       0.88      0.88      0.84     39812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)\n",
    "\n",
    "# Initialize the MLPClassifier\n",
    "model = MLPClassifier(hidden_layer_sizes=(50,), max_iter=200, random_state=1)\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "# Predict on the training and testing sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print training scores\n",
    "print(\"Training Scores:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_train, y_train_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_train, y_train_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_train, y_train_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_train, y_train_pred)}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "# Calculate and print testing scores\n",
    "print(\"\\nTesting Scores:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_test_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_test_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_test_pred)}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.87212398 0.87212398 0.87214589 0.87214589 0.87212077]\n",
      "Mean CV Score: 0.8721321029465955\n"
     ]
    }
   ],
   "source": [
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X,y)\n",
    "merged_data['pred'] = model.predict(X)\n",
    "\n",
    "model = LogisticRegression()\n",
    "# Define cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "# Perform cross-validation\n",
    "y_pred = cross_val_predict(model, X, y, cv=cv)\n",
    "\n",
    "# If you want to see the cross-validation scores for each fold\n",
    "cv_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "print(f\"Mean CV Score: {cv_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.9994474  0.99957299 0.99937203 0.99994976 0.9989199 ]\n",
      "Mean CV Score: 0.9994524175710439\n"
     ]
    }
   ],
   "source": [
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "# Define cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Perform cross-validation\n",
    "y_pred = cross_val_predict(model, X, y, cv=cv)\n",
    "\n",
    "# If you want to see the cross-validation scores for each fold\n",
    "cv_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "print(f\"Mean CV Score: {cv_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [1.         1.         1.         1.         0.99962322]\n",
      "Mean CV Score: 0.9999246439426288\n"
     ]
    }
   ],
   "source": [
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "\n",
    "model = GradientBoostingClassifier()\n",
    "# Define cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "# Perform cross-validation\n",
    "y_pred = cross_val_predict(model, X, y, cv=cv)\n",
    "\n",
    "# If you want to see the cross-validation scores for each fold\n",
    "cv_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "print(f\"Mean CV Score: {cv_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.89267055 0.90329549 0.89048253 0.9032177  0.83032328]\n",
      "Mean CV Score: 0.8839979102959464\n"
     ]
    }
   ],
   "source": [
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes=(50,), max_iter=200, random_state=1, early_stopping=True)\n",
    "# Define cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "# Perform cross-validation\n",
    "y_pred = cross_val_predict(model, X, y, cv=cv)\n",
    "\n",
    "# If you want to see the cross-validation scores for each fold\n",
    "cv_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "print(f\"Mean CV Score: {cv_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Imbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159245, 40) (39812, 40) (159245,) (39812,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After OverSampling, the shape of train_X: (277738, 40)\n",
      "After OverSampling, the shape of train_y: (277738,) \n",
      "\n",
      "After OverSampling, counts of label '1': 138869\n",
      "After OverSampling, counts of label '0': 138869\n"
     ]
    }
   ],
   "source": [
    "# Fit and resample the training data\n",
    "# Print the shapes of the resampled data\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_resampled.shape))\n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_resampled.shape))\n",
    "\n",
    "# Print the counts of each class\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_resampled == 1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_resampled == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5612880538531096\n",
      "Precision_score: 0.15376446257895032\n",
      "Recall_score: 0.5418554264329328\n",
      "F1_score: 0.23955067920585163\n",
      "Classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.56      0.69     34735\n",
      "         1.0       0.15      0.54      0.24      5077\n",
      "\n",
      "    accuracy                           0.56     39812\n",
      "   macro avg       0.52      0.55      0.47     39812\n",
      "weighted avg       0.80      0.56      0.63     39812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 72)\n",
    "\n",
    "ros = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train the model on the training set\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate and print metrics\n",
    "print(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\n",
    "print(f\"Precision_score: {precision_score(y_test, predictions, average='binary')}\")\n",
    "print(f\"Recall_score: {recall_score(y_test, predictions, average='binary')}\")\n",
    "print(f\"F1_score: {f1_score(y_test, predictions, average='binary')}\")\n",
    "print('Classification_report:')\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9996483472319904\n",
      "Precision_score: 0.9996052891257154\n",
      "Recall_score: 0.9976363994484932\n",
      "F1_score: 0.9986198738170347\n",
      "Classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     34735\n",
      "         1.0       1.00      1.00      1.00      5077\n",
      "\n",
      "    accuracy                           1.00     39812\n",
      "   macro avg       1.00      1.00      1.00     39812\n",
      "weighted avg       1.00      1.00      1.00     39812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 72)\n",
    "\n",
    "ros = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Fit the model\n",
    "rf1 = RandomForestClassifier()\n",
    "rf1.fit(X_train_resampled, y_train_resampled)\n",
    "# Make predictions\n",
    "predictions = rf1.predict(X_test)\n",
    "\n",
    "# Calculate and print metrics\n",
    "print(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\n",
    "print(f\"Precision_score: {precision_score(y_test, predictions, average='binary')}\")\n",
    "print(f\"Recall_score: {recall_score(y_test, predictions, average='binary')}\")\n",
    "print(f\"F1_score: {f1_score(y_test, predictions, average='binary')}\")\n",
    "print('Classification_report:')\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9999497638902843\n",
      "Precision_score: 0.9996062216971845\n",
      "Recall_score: 1.0\n",
      "F1_score: 0.9998030720756204\n",
      "Classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     34735\n",
      "         1.0       1.00      1.00      1.00      5077\n",
      "\n",
      "    accuracy                           1.00     39812\n",
      "   macro avg       1.00      1.00      1.00     39812\n",
      "weighted avg       1.00      1.00      1.00     39812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 72)\n",
    "\n",
    "ros = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Fit the model\n",
    "rf1 = GradientBoostingClassifier()\n",
    "rf1.fit(X_train_resampled, y_train_resampled)\n",
    "# Make predictions\n",
    "predictions = rf1.predict(X_test)\n",
    "\n",
    "# Calculate and print metrics\n",
    "print(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\n",
    "print(f\"Precision_score: {precision_score(y_test, predictions, average='binary')}\")\n",
    "print(f\"Recall_score: {recall_score(y_test, predictions, average='binary')}\")\n",
    "print(f\"F1_score: {f1_score(y_test, predictions, average='binary')}\")\n",
    "print('Classification_report:')\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7776047422887572\n",
      "Precision_score: 0.35705094239648777\n",
      "Recall_score: 0.9290919834547962\n",
      "F1_score: 0.5158573928258967\n",
      "Classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.76      0.86     34735\n",
      "         1.0       0.36      0.93      0.52      5077\n",
      "\n",
      "    accuracy                           0.78     39812\n",
      "   macro avg       0.67      0.84      0.69     39812\n",
      "weighted avg       0.91      0.78      0.81     39812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 72)\n",
    "\n",
    "ros = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "# Fit the model\n",
    "MLP = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, random_state=42, early_stopping=True)\n",
    "MLP.fit(X_train_resampled, y_train_resampled)\n",
    "# Make predictions\n",
    "predictions = MLP.predict(X_test)\n",
    "\n",
    "# Calculate and print metrics\n",
    "print(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\n",
    "print(f\"Precision_score: {precision_score(y_test, predictions, average='binary')}\")\n",
    "print(f\"Recall_score: {recall_score(y_test, predictions, average='binary')}\")\n",
    "print(f\"F1_score: {f1_score(y_test, predictions, average='binary')}\")\n",
    "print('Classification_report:')\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UnderSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159245, 40) (39812, 40) (159245,) (39812,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After OverSampling, the shape of train_X: (40676, 40)\n",
      "After OverSampling, the shape of train_y: (40676,) \n",
      "\n",
      "After OverSampling, counts of label '1': 20338\n",
      "After OverSampling, counts of label '0': 20338\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the SMOTE object\n",
    "sm = RandomUnderSampler(random_state=1)\n",
    "# Fit and resample the training data\n",
    "X_train_under, y_train_under = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Print the shapes of the resampled data\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_under.shape))\n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_under.shape))\n",
    "\n",
    "# Print the counts of each class\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_under == 1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_under == 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5772631367426907\n",
      "Precision_score: 0.1577163559866768\n",
      "Recall_score: 0.5276637341153471\n",
      "F1_score: 0.2428468598164477\n",
      "Classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.58      0.71     34697\n",
      "         1.0       0.16      0.53      0.24      5115\n",
      "\n",
      "    accuracy                           0.58     39812\n",
      "   macro avg       0.53      0.56      0.47     39812\n",
      "weighted avg       0.80      0.58      0.65     39812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)\n",
    "\n",
    "# Instantiate the SMOTE object\n",
    "sm = RandomUnderSampler(random_state=1)\n",
    "# Fit and resample the training data\n",
    "X_train_under, y_train_under = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "lr1 = LogisticRegression() \n",
    "lr1.fit(X_train_under, y_train_under)\n",
    "predictions = lr1.predict(X_test) \n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\n",
    "print(f\"Precision_score: {precision_score(y_test, predictions, average='binary')}\")\n",
    "print(f\"Recall_score: {recall_score(y_test, predictions, average='binary')}\")\n",
    "print(f\"F1_score: {f1_score(y_test, predictions, average='binary')}\")\n",
    "print('Classification_report:')\n",
    "# print classification report \n",
    "print(classification_report(y_test, predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9999246458354265\n",
      "Precision_score: 0.9998044583496285\n",
      "Recall_score: 0.9996089931573803\n",
      "F1_score: 0.999706716199042\n",
      "Classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     34697\n",
      "         1.0       1.00      1.00      1.00      5115\n",
      "\n",
      "    accuracy                           1.00     39812\n",
      "   macro avg       1.00      1.00      1.00     39812\n",
      "weighted avg       1.00      1.00      1.00     39812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)\n",
    "\n",
    "# Instantiate the SMOTE object\n",
    "sm = RandomUnderSampler(random_state=1)\n",
    "# Fit and resample the training data\n",
    "X_train_under, y_train_under = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Fit the model\n",
    "rf1 = RandomForestClassifier()\n",
    "rf1.fit(X_train_under, y_train_under)\n",
    "# Make predictions\n",
    "predictions = rf1.predict(X_test)\n",
    "\n",
    "# Calculate and print metrics\n",
    "print(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\n",
    "print(f\"Precision_score: {precision_score(y_test, predictions, average='binary')}\")\n",
    "print(f\"Recall_score: {recall_score(y_test, predictions, average='binary')}\")\n",
    "print(f\"F1_score: {f1_score(y_test, predictions, average='binary')}\")\n",
    "print('Classification_report:')\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Precision_score: 1.0\n",
      "Recall_score: 1.0\n",
      "F1_score: 1.0\n",
      "Classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     34697\n",
      "         1.0       1.00      1.00      1.00      5115\n",
      "\n",
      "    accuracy                           1.00     39812\n",
      "   macro avg       1.00      1.00      1.00     39812\n",
      "weighted avg       1.00      1.00      1.00     39812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)\n",
    "\n",
    "# Instantiate the SMOTE object\n",
    "sm = RandomUnderSampler(random_state=1)\n",
    "# Fit and resample the training data\n",
    "X_train_under, y_train_under = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Fit the model\n",
    "rf1 = GradientBoostingClassifier()\n",
    "rf1.fit(X_train_under, y_train_under)\n",
    "# Make predictions\n",
    "predictions = rf1.predict(X_test)\n",
    "\n",
    "# Calculate and print metrics\n",
    "print(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\n",
    "print(f\"Precision_score: {precision_score(y_test, predictions, average='binary')}\")\n",
    "print(f\"Recall_score: {recall_score(y_test, predictions, average='binary')}\")\n",
    "print(f\"F1_score: {f1_score(y_test, predictions, average='binary')}\")\n",
    "print('Classification_report:')\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6044408720988647\n",
      "Precision_score: 0.22576984577294062\n",
      "Recall_score: 0.8557184750733138\n",
      "F1_score: 0.3572769569831034\n",
      "Classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.57      0.71     34697\n",
      "         1.0       0.23      0.86      0.36      5115\n",
      "\n",
      "    accuracy                           0.60     39812\n",
      "   macro avg       0.59      0.71      0.54     39812\n",
      "weighted avg       0.87      0.60      0.67     39812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)\n",
    "\n",
    "# Instantiate the SMOTE object\n",
    "sm = RandomUnderSampler(random_state=1)\n",
    "# Fit and resample the training data\n",
    "X_train_under, y_train_under = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Fit the model\n",
    "MLP1 = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, random_state=42, early_stopping=True)\n",
    "MLP1.fit(X_train_under, y_train_under)\n",
    "# Make predictions\n",
    "predictions = MLP1.predict(X_test)\n",
    "\n",
    "# Calculate and print metrics\n",
    "print(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\n",
    "print(f\"Precision_score: {precision_score(y_test, predictions, average='binary')}\")\n",
    "print(f\"Recall_score: {recall_score(y_test, predictions, average='binary')}\")\n",
    "print(f\"F1_score: {f1_score(y_test, predictions, average='binary')}\")\n",
    "print('Classification_report:')\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underweight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores:\n",
      "Accuracy: 0.5684008917077459\n",
      "Precision: 0.15372946362127196\n",
      "Recall: 0.5281738617366506\n",
      "F1 Score: 0.238144855565655\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.57      0.70    138907\n",
      "         1.0       0.15      0.53      0.24     20338\n",
      "\n",
      "    accuracy                           0.57    159245\n",
      "   macro avg       0.52      0.55      0.47    159245\n",
      "weighted avg       0.80      0.57      0.64    159245\n",
      "\n",
      "\n",
      "Testing Scores:\n",
      "Accuracy: 0.572566060484276\n",
      "Precision: 0.15735836020267158\n",
      "Recall: 0.5343108504398827\n",
      "F1 Score: 0.2431170217497665\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.58      0.70     34697\n",
      "         1.0       0.16      0.53      0.24      5115\n",
      "\n",
      "    accuracy                           0.57     39812\n",
      "   macro avg       0.53      0.56      0.47     39812\n",
      "weighted avg       0.80      0.57      0.64     39812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)\n",
    "\n",
    "# Train the logistic regression model\n",
    "model = LogisticRegression(class_weight='balanced', random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training and testing sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print training scores\n",
    "print(\"Training Scores:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_train, y_train_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_train, y_train_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_train, y_train_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_train, y_train_pred)}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "# Calculate and print testing scores\n",
    "print(\"\\nTesting Scores:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_test_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_test_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_test_pred)}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00    138907\n",
      "         1.0       1.00      1.00      1.00     20338\n",
      "\n",
      "    accuracy                           1.00    159245\n",
      "   macro avg       1.00      1.00      1.00    159245\n",
      "weighted avg       1.00      1.00      1.00    159245\n",
      "\n",
      "\n",
      "Testing Scores:\n",
      "Accuracy: 0.9989450416959711\n",
      "Precision: 0.9998029556650246\n",
      "Recall: 0.9919843597262952\n",
      "F1 Score: 0.9958783120706575\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     34697\n",
      "         1.0       1.00      0.99      1.00      5115\n",
      "\n",
      "    accuracy                           1.00     39812\n",
      "   macro avg       1.00      1.00      1.00     39812\n",
      "weighted avg       1.00      1.00      1.00     39812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "model = RandomForestClassifier(class_weight='balanced', random_state=1)\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "# Predict on the training and testing sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print training scores\n",
    "print(\"Training Scores:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_train, y_train_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_train, y_train_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_train, y_train_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_train, y_train_pred)}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "# Calculate and print testing scores\n",
    "print(\"\\nTesting Scores:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_test_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_test_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_test_pred)}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00    138907\n",
      "         1.0       1.00      1.00      1.00     20338\n",
      "\n",
      "    accuracy                           1.00    159245\n",
      "   macro avg       1.00      1.00      1.00    159245\n",
      "weighted avg       1.00      1.00      1.00    159245\n",
      "\n",
      "\n",
      "Testing Scores:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     34697\n",
      "         1.0       1.00      1.00      1.00      5115\n",
      "\n",
      "    accuracy                           1.00     39812\n",
      "   macro avg       1.00      1.00      1.00     39812\n",
      "weighted avg       1.00      1.00      1.00     39812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)\n",
    "\n",
    "# Compute sample weights\n",
    "sample_weights = compute_sample_weight('balanced', y_train)\n",
    "\n",
    "# Train the model on the training set\n",
    "model = GradientBoostingClassifier(random_state=1)\n",
    "model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Predict on the training and testing sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print training scores\n",
    "print(\"Training Scores:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_train, y_train_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_train, y_train_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_train, y_train_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_train, y_train_pred)}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "# Calculate and print testing scores\n",
    "print(\"\\nTesting Scores:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_test_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_test_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_test_pred)}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BaseMultilayerPerceptron.fit() got an unexpected keyword argument 'sample_weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Train the model on the training set\u001b[39;00m\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m MLPClassifier(hidden_layer_sizes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m100\u001b[39m,), max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Predict on the training and testing sets\u001b[39;00m\n\u001b[0;32m     13\u001b[0m y_train_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train)\n",
      "File \u001b[1;32md:\\Data Science\\Projects\\My Projects\\Project 12\\.venv\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: BaseMultilayerPerceptron.fit() got an unexpected keyword argument 'sample_weight'"
     ]
    }
   ],
   "source": [
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)\n",
    "\n",
    "# Compute sample weights\n",
    "sample_weights = compute_sample_weight('balanced', y_train)\n",
    "\n",
    "# Train the model on the training set\n",
    "model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, random_state=42, early_stopping=True)\n",
    "model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Predict on the training and testing sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print training scores\n",
    "print(\"Training Scores:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_train, y_train_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_train, y_train_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_train, y_train_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_train, y_train_pred)}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "# Calculate and print testing scores\n",
    "print(\"\\nTesting Scores:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_test_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_test_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_test_pred)}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're right; MLPClassifier does not directly accept the sample_weight parameter in the fit method. Instead, we need to handle class imbalance differently for MLPClassifier. One common approach is to manually balance the dataset during training. This can be done using the resample function from sklearn.utils."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination (RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Ranking:  [28 27  3  7  1 30 23 24  1  1  2  1  1  8 15  6 21 25 22  1  1  1 29 16\n",
      " 26  1 11 19 12 20 18 31 10  1  5 17 13 14  4  9]\n",
      "Selected Features:  Index(['decimalPrice', 'position', 'positionL', 'weightSt', 'weightLb',\n",
      "       'runners', 'margin', 'weight', 'rclass', 'class'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset\n",
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "model = LogisticRegression(max_iter=500)\n",
    "\n",
    "# Perform RFE\n",
    "selector = RFE(model, n_features_to_select=10)  # Select the top 10 features\n",
    "selector = selector.fit(X_train, y_train)\n",
    "\n",
    "# Print the ranking of features\n",
    "print(\"Feature Ranking: \", selector.ranking_) \n",
    "# Print the selected features\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "print(\"Selected Features: \", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Ranking:  [ 4  8 22  1  1 31 14 12  1  1  1 25 21  1  1  2 10 13 11  1  1  5 29 16\n",
      "  9 17 26 27  1  3  7 30 28 15 19 24 23  6 20 18]\n",
      "Selected Features:  Index(['saddle', 'decimalPrice', 'position', 'positionL', 'dist', 'RPR', 'TR',\n",
      "       'runners', 'margin', 'winningTime'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset\n",
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the random forest classifier\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1, n_estimators=100)\n",
    "\n",
    "# Perform RFE\n",
    "selector = RFE(model, n_features_to_select=10)  # Select the top 10 features\n",
    "selector = selector.fit(X_train, y_train)\n",
    "\n",
    "# Transform the data to the selected features\n",
    "X_train_rfe = selector.transform(X_train)\n",
    "X_test_rfe = selector.transform(X_test)\n",
    "\n",
    "# Train the model on the selected features\n",
    "model.fit(X_train_rfe, y_train)\n",
    "\n",
    "# Print the ranking of features\n",
    "print(\"Feature Ranking: \", selector.ranking_)\n",
    "\n",
    "# Print the selected features\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "print(\"Selected Features: \", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset\n",
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the gradient boosting classifier with optimized parameters\n",
    "model = GradientBoostingClassifier(random_state=42, n_estimators=50, max_depth=3)  # Adjust these parameters as needed\n",
    "\n",
    "# Perform RFE\n",
    "selector = RFE(model, n_features_to_select=10)  # Select the top 10 features\n",
    "selector = selector.fit(X_train, y_train)\n",
    "\n",
    "# Transform the data to the selected features\n",
    "X_train_rfe = selector.transform(X_train)\n",
    "X_test_rfe = selector.transform(X_test)\n",
    "\n",
    "# Train the model on the selected features\n",
    "model.fit(X_train_rfe, y_train)\n",
    "\n",
    "# Print the ranking of features\n",
    "print(\"Feature Ranking: \", selector.ranking_)\n",
    "\n",
    "# Print the selected features\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "print(\"Selected Features: \", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9840811328456153\n",
      "Train Precision: 0.983938920026233\n",
      "Train Recall: 0.9840811328456153\n",
      "Train F1 Score: 0.9839792938946765\n",
      "Train Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99    138907\n",
      "         1.0       0.95      0.92      0.94     20338\n",
      "\n",
      "    accuracy                           0.98    159245\n",
      "   macro avg       0.97      0.96      0.96    159245\n",
      "weighted avg       0.98      0.98      0.98    159245\n",
      "\n",
      "Test Accuracy: 0.9839997990555611\n",
      "Test Precision: 0.983861568471008\n",
      "Test Recall: 0.9839997990555611\n",
      "Test F1 Score: 0.9839028362067743\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99     34697\n",
      "         1.0       0.95      0.92      0.94      5115\n",
      "\n",
      "    accuracy                           0.98     39812\n",
      "   macro avg       0.97      0.96      0.96     39812\n",
      "weighted avg       0.98      0.98      0.98     39812\n",
      "\n",
      "Selected Features:  Index(['decimalPrice', 'position', 'dist', 'weightSt', 'RPR', 'TR', 'runners',\n",
      "       'weight', 'winningTime', 'metric'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset\n",
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform feature selection using SelectKBest\n",
    "selector = SelectKBest(score_func=f_classif, k=10)  # Select the top 10 features\n",
    "X_train_kbest = selector.fit_transform(X_train, y_train)\n",
    "X_test_kbest = selector.transform(X_test)\n",
    "\n",
    "# Initialize the MLPClassifier with optimized parameters\n",
    "model = MLPClassifier(hidden_layer_sizes=(50,), max_iter=100, random_state=42, early_stopping=True, n_iter_no_change=10)\n",
    "\n",
    "# Train the model on the selected features\n",
    "model.fit(X_train_kbest, y_train)\n",
    "\n",
    "# Print the selected features\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "print(\"Selected Features: \", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization techniques to reduce model complexity and prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores:\n",
      "Accuracy: 0.9999120851518101\n",
      "Precision: 0.999312106918239\n",
      "Recall: 1.0\n",
      "F1 Score: 0.9996559351191939\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00    138907\n",
      "         1.0       1.00      1.00      1.00     20338\n",
      "\n",
      "    accuracy                           1.00    159245\n",
      "   macro avg       1.00      1.00      1.00    159245\n",
      "weighted avg       1.00      1.00      1.00    159245\n",
      "\n",
      "\n",
      "Testing Scores:\n",
      "Accuracy: 0.9999748819451422\n",
      "Precision: 0.9998045347928068\n",
      "Recall: 1.0\n",
      "F1 Score: 0.999902257843808\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     34697\n",
      "         1.0       1.00      1.00      1.00      5115\n",
      "\n",
      "    accuracy                           1.00     39812\n",
      "   macro avg       1.00      1.00      1.00     39812\n",
      "weighted avg       1.00      1.00      1.00     39812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data\n",
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Training the model with L2 regularization\n",
    "model = LogisticRegression(penalty='l2', C=1.0, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training and testing sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print training scores\n",
    "print(\"Training Scores:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_train, y_train_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_train, y_train_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_train, y_train_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_train, y_train_pred)}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "# Calculate and print testing scores\n",
    "print(\"\\nTesting Scores:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_test_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_test_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_test_pred)}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores:\n",
      "Accuracy: 0.9999120851518101\n",
      "Precision: 0.999312106918239\n",
      "Recall: 1.0\n",
      "F1 Score: 0.9996559351191939\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00    138907\n",
      "         1.0       1.00      1.00      1.00     20338\n",
      "\n",
      "    accuracy                           1.00    159245\n",
      "   macro avg       1.00      1.00      1.00    159245\n",
      "weighted avg       1.00      1.00      1.00    159245\n",
      "\n",
      "\n",
      "Testing Scores:\n",
      "Accuracy: 0.9999748819451422\n",
      "Precision: 0.9998045347928068\n",
      "Recall: 1.0\n",
      "F1 Score: 0.999902257843808\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     34697\n",
      "         1.0       1.00      1.00      1.00      5115\n",
      "\n",
      "    accuracy                           1.00     39812\n",
      "   macro avg       1.00      1.00      1.00     39812\n",
      "weighted avg       1.00      1.00      1.00     39812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data\n",
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Training the model with L1 regularization\n",
    "model = LogisticRegression(penalty='l1', C=1.0, solver='liblinear', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training and testing sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print training scores\n",
    "print(\"Training Scores:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_train, y_train_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_train, y_train_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_train, y_train_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_train, y_train_pred)}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "# Calculate and print testing scores\n",
    "print(\"\\nTesting Scores:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_test_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_test_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_test_pred)}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores:\n",
      "Accuracy: 0.9631008822882979\n",
      "Precision: 0.9273640661938535\n",
      "Recall: 0.7715114563870588\n",
      "F1 Score: 0.8422889043963713\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.99      0.98    138907\n",
      "         1.0       0.93      0.77      0.84     20338\n",
      "\n",
      "    accuracy                           0.96    159245\n",
      "   macro avg       0.95      0.88      0.91    159245\n",
      "weighted avg       0.96      0.96      0.96    159245\n",
      "\n",
      "\n",
      "Testing Scores:\n",
      "Accuracy: 0.9631015774138451\n",
      "Precision: 0.9255368814192344\n",
      "Recall: 0.7751710654936461\n",
      "F1 Score: 0.8437067773167358\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.99      0.98     34697\n",
      "         1.0       0.93      0.78      0.84      5115\n",
      "\n",
      "    accuracy                           0.96     39812\n",
      "   macro avg       0.95      0.88      0.91     39812\n",
      "weighted avg       0.96      0.96      0.96     39812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data\n",
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Training the model with Elastic Net regularization\n",
    "model = LogisticRegression(penalty='elasticnet', l1_ratio=0.5, C=1.0, solver='saga', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training and testing sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print training scores\n",
    "print(\"Training Scores:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_train, y_train_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_train, y_train_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_train, y_train_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_train, y_train_pred)}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "# Calculate and print testing scores\n",
    "print(\"\\nTesting Scores:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_test_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_test_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_test_pred)}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Evaluation:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00    138907\n",
      "         1.0       1.00      1.00      1.00     20338\n",
      "\n",
      "    accuracy                           1.00    159245\n",
      "   macro avg       1.00      1.00      1.00    159245\n",
      "weighted avg       1.00      1.00      1.00    159245\n",
      "\n",
      "Test Set Evaluation:\n",
      "Accuracy: 0.9996232291771325\n",
      "Precision: 0.9998039984319874\n",
      "Recall: 0.9972629521016618\n",
      "F1 Score: 0.9985318586669276\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     34697\n",
      "         1.0       1.00      1.00      1.00      5115\n",
      "\n",
      "    accuracy                           1.00     39812\n",
      "   macro avg       1.00      1.00      1.00     39812\n",
      "weighted avg       1.00      1.00      1.00     39812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data\n",
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the RandomForestClassifier with regularization parameters\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,            # Number of trees in the forest\n",
    "    max_depth=None,              # Maximum depth of the tree\n",
    "    min_samples_split=2,         # Minimum number of samples required to split an internal node\n",
    "    min_samples_leaf=1,          # Minimum number of samples required to be at a leaf node\n",
    "    max_features='sqrt',         # Number of features to consider when looking for the best split\n",
    "    bootstrap=True,              # Whether bootstrap samples are used when building trees\n",
    "    random_state=42              # Random seed\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on training set\n",
    "y_pred_train = model.predict(X_train)\n",
    "\n",
    "# Calculate and print metrics for the training set\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "precision_train = precision_score(y_train, y_pred_train)\n",
    "recall_train = recall_score(y_train, y_pred_train)\n",
    "f1_train = f1_score(y_train, y_pred_train)\n",
    "\n",
    "print(\"Training Set Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_train}\")\n",
    "print(f\"Precision: {precision_train}\")\n",
    "print(f\"Recall: {recall_train}\")\n",
    "print(f\"F1 Score: {f1_train}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "print(\"Test Set Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_test)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_test)}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_test)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_test)}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Evaluation:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00    138907\n",
      "         1.0       1.00      1.00      1.00     20338\n",
      "\n",
      "    accuracy                           1.00    159245\n",
      "   macro avg       1.00      1.00      1.00    159245\n",
      "weighted avg       1.00      1.00      1.00    159245\n",
      "\n",
      "Test Set Evaluation:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     34697\n",
      "         1.0       1.00      1.00      1.00      5115\n",
      "\n",
      "    accuracy                           1.00     39812\n",
      "   macro avg       1.00      1.00      1.00     39812\n",
      "weighted avg       1.00      1.00      1.00     39812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data\n",
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the GradientBoostingClassifier with regularization parameters\n",
    "model = GradientBoostingClassifier(\n",
    "    n_estimators=100,            # Number of boosting stages\n",
    "    learning_rate=0.1,           # Learning rate shrinks the contribution of each tree\n",
    "    max_depth=3,                 # Maximum depth of the individual trees\n",
    "    min_samples_split=2,         # Minimum number of samples required to split an internal node\n",
    "    min_samples_leaf=1,          # Minimum number of samples required to be at a leaf node\n",
    "    max_features=None,           # Number of features to consider when looking for the best split\n",
    "    random_state=42              # Random seed\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on training set\n",
    "y_pred_train = model.predict(X_train)\n",
    "\n",
    "# Calculate and print metrics for the training set\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "precision_train = precision_score(y_train, y_pred_train)\n",
    "recall_train = recall_score(y_train, y_pred_train)\n",
    "f1_train = f1_score(y_train, y_pred_train)\n",
    "\n",
    "print(\"Training Set Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_train}\")\n",
    "print(f\"Precision: {precision_train}\")\n",
    "print(f\"Recall: {recall_train}\")\n",
    "print(f\"F1 Score: {f1_train}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "print(\"Test Set Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_test)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_test)}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_test)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_test)}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Evaluation:\n",
      "Accuracy: 0.8973845332663506\n",
      "Precision: 0.8886312851361486\n",
      "Recall: 0.8973845332663506\n",
      "F1 Score: 0.8915995613530152\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.96      0.94    138907\n",
      "         1.0       0.63      0.48      0.55     20338\n",
      "\n",
      "    accuracy                           0.90    159245\n",
      "   macro avg       0.78      0.72      0.74    159245\n",
      "weighted avg       0.89      0.90      0.89    159245\n",
      "\n",
      "Test Set Evaluation:\n",
      "Accuracy: 0.8961870792725811\n",
      "Precision: 0.887868073255074\n",
      "Recall: 0.8961870792725811\n",
      "F1 Score: 0.8908277678008784\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.96      0.94     34697\n",
      "         1.0       0.62      0.49      0.55      5115\n",
      "\n",
      "    accuracy                           0.90     39812\n",
      "   macro avg       0.77      0.72      0.74     39812\n",
      "weighted avg       0.89      0.90      0.89     39812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data\n",
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the MLPClassifier with regularization parameters\n",
    "mlp_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(100,),  # One hidden layer with 100 neurons\n",
    "    activation='relu',          # Activation function for the hidden layer\n",
    "    solver='adam',              # Solver for weight optimization\n",
    "    alpha=0.0001,               # L2 regularization parameter\n",
    "    batch_size='auto',          # Size of minibatches for stochastic optimizers\n",
    "    learning_rate='constant',   # Learning rate schedule for weight updates\n",
    "    learning_rate_init=0.001,   # Initial learning rate\n",
    "    max_iter=200,               # Maximum number of iterations\n",
    "    shuffle=True,               # Whether to shuffle samples in each iteration\n",
    "    random_state=42,            # Random seed\n",
    "    tol=0.0001,                 # Tolerance for the optimization\n",
    "    early_stopping=True,        # Whether to use early stopping to terminate training when validation score is not improving\n",
    "    validation_fraction=0.1,    # Proportion of training data to set aside as validation set for early stopping\n",
    "    n_iter_no_change=10,        # Number of iterations with no improvement to wait before stopping\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on training set\n",
    "y_pred_train = mlp_model.predict(X_train)\n",
    "\n",
    "# Calculate and print metrics for the training set\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "precision_train = precision_score(y_train, y_pred_train, average='weighted')\n",
    "recall_train = recall_score(y_train, y_pred_train, average='weighted')\n",
    "f1_train = f1_score(y_train, y_pred_train, average='weighted')\n",
    "\n",
    "print(\"Training Set Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_train}\")\n",
    "print(f\"Precision: {precision_train}\")\n",
    "print(f\"Recall: {recall_train}\")\n",
    "print(f\"F1 Score: {f1_train}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred_test = mlp_model.predict(X_test)\n",
    "\n",
    "print(\"Test Set Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_test)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_test, average='weighted')}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_test, average='weighted')}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_test, average='weighted')}\")\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'C': 0.001, 'penalty': 'l2'}\n",
      "Best cross-validation score: 0.87\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data\n",
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "\n",
    "# Define the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],          # Regularization penalty: l1 or l2\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100] # Regularization parameter\n",
    "}\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'C': 0.7337391594646552, 'penalty': 'l2'}\n",
      "Best cross-validation score: 0.87\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data\n",
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "\n",
    "# Define the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Define the parameter distribution\n",
    "param_dist = {\n",
    "    'penalty': ['l1', 'l2'],                      # Regularization penalty: l1 or l2\n",
    "    'C': uniform(loc=0, scale=4)                  # Regularization parameter (uniform distribution)\n",
    "}\n",
    "\n",
    "# Instantiate the random search model\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=100, cv=5, scoring='accuracy', random_state=42)\n",
    "\n",
    "# Fit the random search to the training data\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(random_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'bootstrap': False, 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Best cross-validation score: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data\n",
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "\n",
    "# Define the model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Define a smaller parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [None, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Define cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Perform grid search with parallel computation\n",
    "grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Perform cross-validation with the best model\n",
    "y_pred = cross_val_predict(best_model, X, y, cv=cv, n_jobs=-1)\n",
    "\n",
    "# Calculate and print the metrics\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 20, 'bootstrap': True}\n",
      "Best cross-validation score: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data\n",
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "\n",
    "# Define the model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Define the parameter distribution with narrower ranges\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 8],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Define cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Perform random search with fewer iterations and parallel computation\n",
    "random_search = RandomizedSearchCV(model, param_dist, n_iter=5, cv=cv, scoring='accuracy', random_state=42, n_jobs=-1)\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Get the best model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Perform cross-validation with the best model\n",
    "y_pred = cross_val_predict(best_model, X, y, cv=cv, n_jobs=-1)\n",
    "\n",
    "# Calculate and print the metrics\n",
    "print(f\"Best Parameters: {random_search.best_params_}\")\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoosting Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 4],\n",
    "    'subsample': [0.9, 1.0],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "# Initialize the Gradient Boosting classifier and GridSearchCV\n",
    "gbc = GradientBoostingClassifier()\n",
    "grid_search = GridSearchCV(estimator=gbc, param_grid=param_grid, n_jobs=-1,cv=5,scoring='accuracy')\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best parameters found by GridSearchCV\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoost Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "\n",
    "# Define parameter distribution\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(100, 301, 50),  # Range from 100 to 300 with step 50\n",
    "    'learning_rate': np.linspace(0.01, 0.1, 10),  # 10 values between 0.01 and 0.1\n",
    "    'max_depth': np.arange(3, 6),  # Range from 3 to 5\n",
    "    'subsample': np.linspace(0.8, 1.0, 3),  # 3 values between 0.8 and 1.0\n",
    "    'min_samples_split': np.arange(2, 11, 3)  # Range from 2 to 10 with step 3\n",
    "}\n",
    "\n",
    "# Initialize the Gradient Boosting classifier and RandomizedSearchCV\n",
    "gbc = GradientBoostingClassifier()\n",
    "random_search = RandomizedSearchCV(estimator=gbc, param_distributions=param_dist, n_iter=100, cv=5, n_jobs=-1, verbose=2, scoring='accuracy', random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Best estimator\n",
    "best_gbc = random_search.best_estimator_\n",
    "# Best parameters found by RandomizedSearchCV\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(random_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "\n",
    "# Define the model\n",
    "model = MLPClassifier()\n",
    "\n",
    "# Define the parameter grid with fewer combinations\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['adam'], \n",
    "    'alpha': [0.0001, 0.001],\n",
    "    'learning_rate': ['constant','adaptive']\n",
    "}\n",
    "\n",
    "# Define cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Perform grid search with fewer combinations and parallel computation\n",
    "grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Calculate and print the metrics\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP RandonSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "X = merged_data.drop(columns=['res_place'])\n",
    "y = merged_data['res_place']\n",
    "\n",
    "# Define the model\n",
    "model = MLPClassifier()\n",
    "\n",
    "# Define the parameter distribution\n",
    "param_dist = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (150,),],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': np.logspace(-4, 0, 10),\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "\n",
    "# Define cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Perform random search with fewer iterations and parallel computation\n",
    "random_search = RandomizedSearchCV(model, param_dist, n_iter=20, cv=cv, scoring='accuracy', random_state=42, n_jobs=-1)\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Get the best model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Calculate and print the metrics\n",
    "print(f\"Best Parameters: {random_search.best_params_}\")\n",
    "print(\"Best cross-validation score: {:.2f}\".format(random_search.best_score_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
